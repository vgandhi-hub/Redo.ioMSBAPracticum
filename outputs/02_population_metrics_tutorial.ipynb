{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Population Metrics Configuration and Execution Tutorial\n",
    "\n",
    "**Purpose:** Walk through configuring and running the population_metrics repo\n",
    "\n",
    "**What this notebook does:**\n",
    "1. Shows current config settings\n",
    "2. Validates data files exist\n",
    "3. Runs population metrics\n",
    "4. Displays results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T16:51:42.150506Z",
     "start_time": "2025-11-11T16:51:41.380903Z"
    }
   },
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import subprocess"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T16:51:44.675128Z",
     "start_time": "2025-11-11T16:51:44.671918Z"
    }
   },
   "source": [
    "# Add population_metrics to path\n",
    "project_root = Path.cwd().parent\n",
    "pop_metrics_dir = project_root / \"external_repos\" / \"population_metrics\"\n",
    "sys.path.insert(0, str(pop_metrics_dir))"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: View Current Configuration"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T16:51:48.618112Z",
     "start_time": "2025-11-11T16:51:48.610321Z"
    }
   },
   "source": [
    "# Import config\n",
    "import config_custom as CFG"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T16:51:56.420007Z",
     "start_time": "2025-11-11T16:51:56.415665Z"
    }
   },
   "source": [
    "# Show file paths\n",
    "print(\"File Paths:\")\n",
    "for key, value in CFG.PATHS.items():\n",
    "    print(f\"  {key}: {value}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Paths:\n",
      "  demographics: ../../data/demographics.csv\n",
      "  current_commitments: ../../data/current_commitments_clean.csv\n",
      "  prior_commitments: ../../data/prior_commitments_clean.csv\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T16:52:09.000308Z",
     "start_time": "2025-11-11T16:52:08.997231Z"
    }
   },
   "source": [
    "# Show column mappings\n",
    "print(\"\\nColumn Mappings:\")\n",
    "for key, value in CFG.COLS.items():\n",
    "    print(f\"  {key}: {value}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Column Mappings:\n",
      "  id: cdcno\n",
      "  current_sentence_months: aggregate sentence in months\n",
      "  completed_months: None\n",
      "  past_time_months: None\n",
      "  offense_begin_date: offense begin date\n",
      "  release_date: expected release date\n",
      "  age_years: None\n",
      "  offense_code: offense_clean\n",
      "  offense_description: offense description\n",
      "  offense_category: offense category\n",
      "  in_prison: in prison\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T16:52:16.421061Z",
     "start_time": "2025-11-11T16:52:16.415882Z"
    }
   },
   "source": [
    "# Show offense lists\n",
    "print(\"\\nOffense Classifications:\")\n",
    "print(f\"  Violent codes: {len(CFG.OFFENSE_LISTS.get('violent', []))} codes\")\n",
    "print(f\"    Sample: {CFG.OFFENSE_LISTS.get('violent', [])[:5]}\")\n",
    "print(f\"  Nonviolent codes: {len(CFG.OFFENSE_LISTS.get('nonviolent', []))} codes\")\n",
    "print(f\"    Sample: {CFG.OFFENSE_LISTS.get('nonviolent', [])[:5]}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Offense Classifications:\n",
      "  Violent codes: 83 codes\n",
      "    Sample: ['187', '190.05', '190.25', '190(d)', '190(c)']\n",
      "  Nonviolent codes: 51 codes\n",
      "    Sample: ['136.1', '210.5', '244', '245.2', '245.3']\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T16:52:24.022104Z",
     "start_time": "2025-11-11T16:52:24.018622Z"
    }
   },
   "source": [
    "# Show metric weights\n",
    "print(\"\\nMetric Weights:\")\n",
    "for metric, weight in CFG.METRIC_WEIGHTS.items():\n",
    "    direction = \"GOOD\" if weight > 0 else \"BAD\"\n",
    "    print(f\"  {metric}: {weight:+.1f} ({direction})\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metric Weights:\n",
      "  desc_nonvio_curr: +1.0 (GOOD)\n",
      "  desc_nonvio_past: +1.0 (GOOD)\n",
      "  severity_trend: +1.0 (GOOD)\n",
      "  age: -0.5 (BAD)\n",
      "  freq_violent: -1.0 (BAD)\n",
      "  freq_total: -0.5 (BAD)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Validate Data Files"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T16:52:35.121260Z",
     "start_time": "2025-11-11T16:52:32.936612Z"
    }
   },
   "source": [
    "# Check if files exist\n",
    "print(\"Checking data files...\\n\")\n",
    "\n",
    "for name, path in CFG.PATHS.items():\n",
    "    full_path = pop_metrics_dir / path\n",
    "    exists = full_path.exists()\n",
    "    status = \"✓\" if exists else \"✗\"\n",
    "    print(f\"{status} {name}: {full_path}\")\n",
    "    \n",
    "    if exists:\n",
    "        df = pd.read_csv(full_path)\n",
    "        print(f\"  Rows: {len(df):,}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking data files...\n",
      "\n",
      "✓ demographics: C:\\Users\\gandh\\PycharmProjects\\PythonProject\\external_repos\\population_metrics\\..\\..\\data\\demographics.csv\n",
      "  Rows: 95,476\n",
      "✓ current_commitments: C:\\Users\\gandh\\PycharmProjects\\PythonProject\\external_repos\\population_metrics\\..\\..\\data\\current_commitments_clean.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gandh\\AppData\\Local\\Temp\\ipykernel_7532\\1829870776.py:11: DtypeWarning: Columns (7,22,23,24,25,26,27,28,29,30,31,32,33) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(full_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Rows: 369,125\n",
      "✓ prior_commitments: C:\\Users\\gandh\\PycharmProjects\\PythonProject\\external_repos\\population_metrics\\..\\..\\data\\prior_commitments_clean.csv\n",
      "  Rows: 191,436\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Preview Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T16:52:41.062032Z",
     "start_time": "2025-11-11T16:52:40.805875Z"
    }
   },
   "source": [
    "# Load demographics to check columns\n",
    "demo_path = pop_metrics_dir / CFG.PATHS['demographics']\n",
    "demographics = pd.read_csv(demo_path)\n",
    "\n",
    "print(\"Demographics columns:\")\n",
    "for col in demographics.columns:\n",
    "    print(f\"  - {col}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demographics columns:\n",
      "  - cdcno\n",
      "  - ethnicity\n",
      "  - controlling offense\n",
      "  - description\n",
      "  - offense begin date\n",
      "  - offense end date\n",
      "  - controlling case number\n",
      "  - controlling case sentencing county\n",
      "  - sentence type\n",
      "  - aggregate sentence in months\n",
      "  - offense category\n",
      "  - eprd mepd month and year\n",
      "  - current location\n",
      "  - aggregate sentence in years\n",
      "  - time served in years\n",
      "  - expected release date\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T16:53:09.004305Z",
     "start_time": "2025-11-11T16:53:07.910078Z"
    }
   },
   "source": [
    "# Load current commits to check offense_clean column\n",
    "current_path = pop_metrics_dir / CFG.PATHS['current_commitments']\n",
    "current = pd.read_csv(current_path)\n",
    "\n",
    "print(\"\\nCurrent commits - key columns:\")\n",
    "key_cols = ['cdcno', 'offense', 'offense_clean', 'offense description']\n",
    "existing_cols = [col for col in key_cols if col in current.columns]\n",
    "print(current[existing_cols].head(3))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current commits - key columns:\n",
      "        cdcno        offense offense_clean  \\\n",
      "0  2cf2a233c4     VC10851(a)         10851   \n",
      "1  5a72696541  PC191.5(c)(2)         191.5   \n",
      "2  5a72696541      PC187 2nd       187 2ND   \n",
      "\n",
      "                        offense description  \n",
      "0                             Vehicle Theft  \n",
      "1  Vehicular Manslaughter While Intoxicated  \n",
      "2                                Murder 2nd  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gandh\\AppData\\Local\\Temp\\ipykernel_7532\\331520964.py:3: DtypeWarning: Columns (7,22,23,24,25,26,27,28,29,30,31,32,33) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  current = pd.read_csv(current_path)\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Verify Offense Classification Will Work"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T16:53:14.191745Z",
     "start_time": "2025-11-11T16:53:14.162158Z"
    }
   },
   "source": [
    "# Check how many offenses will match violent/nonviolent lists\n",
    "if 'offense_clean' in current.columns:\n",
    "    violent_codes = set(CFG.OFFENSE_LISTS.get('violent', []))\n",
    "    nonviolent_codes = set(CFG.OFFENSE_LISTS.get('nonviolent', []))\n",
    "    \n",
    "    current_codes = current['offense_clean'].dropna().unique()\n",
    "    \n",
    "    violent_matches = sum(1 for code in current_codes if str(code) in violent_codes)\n",
    "    nonviolent_matches = sum(1 for code in current_codes if str(code) in nonviolent_codes)\n",
    "    \n",
    "    print(f\"Offense code matching:\")\n",
    "    print(f\"  Total unique offense codes in data: {len(current_codes)}\")\n",
    "    print(f\"  Matches violent list: {violent_matches}\")\n",
    "    print(f\"  Matches nonviolent list: {nonviolent_matches}\")\n",
    "    print(f\"  Unclassified: {len(current_codes) - violent_matches - nonviolent_matches}\")\n",
    "else:\n",
    "    print(\"Warning: offense_clean column not found!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Offense code matching:\n",
      "  Total unique offense codes in data: 463\n",
      "  Matches violent list: 22\n",
      "  Matches nonviolent list: 21\n",
      "  Unclassified: 420\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Run Population Metrics (Demo Mode)\n",
    "\n",
    "**Note:** This will take 30-60 seconds"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T16:54:14.976076Z",
     "start_time": "2025-11-11T16:54:14.940004Z"
    }
   },
   "source": [
    "# Run via subprocess to show progress\n",
    "# This runs: python scripts/01b_run_population_metrics_fast.py\n",
    "\n",
    "script_path = project_root / \"scripts\" / \"01b_run_population_metrics_fast.py\"\n",
    "\n",
    "print(\"Running population metrics (demo mode)...\\n\")\n",
    "print(\"This will process 5,000 random individuals\\n\")\n",
    "\n",
    "# Note: This runs the script, but you'll need to provide input \"1\" for demo mode\n",
    "# Uncomment the line below to run:\n",
    "result = subprocess.run(['python', str(script_path)], capture_output=True, text=True, input=\"1\\n\")\n",
    "print(result.stdout)\n",
    "\n",
    "print(\"To run, uncomment the subprocess lines above, or run in terminal:\")\n",
    "print(f\"  python {script_path}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running population metrics (demo mode)...\n",
      "\n",
      "This will process 5,000 random individuals\n",
      "\n",
      "\n",
      "To run, uncomment the subprocess lines above, or run in terminal:\n",
      "  python C:\\Users\\gandh\\PycharmProjects\\PythonProject\\scripts\\01b_run_population_metrics_fast.py\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative: Run Population Metrics Directly in Notebook"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T16:54:19.800574Z",
     "start_time": "2025-11-11T16:54:19.775156Z"
    }
   },
   "source": [
    "# Import the modules\n",
    "import compute_metrics as cm\n",
    "import sentencing_math as sm\n",
    "from tqdm import tqdm\n",
    "import random"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T16:54:21.745279Z",
     "start_time": "2025-11-11T16:54:21.468913Z"
    }
   },
   "source": [
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "demo = cm.read_table(pop_metrics_dir / CFG.PATHS[\"demographics\"])\n",
    "cur = cm.read_table(pop_metrics_dir / CFG.PATHS[\"current_commitments\"])\n",
    "pri = cm.read_table(pop_metrics_dir / CFG.PATHS[\"prior_commitments\"])\n",
    "\n",
    "print(f\"Demographics: {len(demo):,} rows\")\n",
    "print(f\"Current commits: {len(cur):,} rows\")\n",
    "print(f\"Prior commits: {len(pri):,} rows\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'WindowsPath' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAttributeError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[16]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# Load data\u001B[39;00m\n\u001B[32m      2\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mLoading data...\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m demo = \u001B[43mcm\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread_table\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpop_metrics_dir\u001B[49m\u001B[43m \u001B[49m\u001B[43m/\u001B[49m\u001B[43m \u001B[49m\u001B[43mCFG\u001B[49m\u001B[43m.\u001B[49m\u001B[43mPATHS\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mdemographics\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      4\u001B[39m cur = cm.read_table(pop_metrics_dir / CFG.PATHS[\u001B[33m\"\u001B[39m\u001B[33mcurrent_commitments\u001B[39m\u001B[33m\"\u001B[39m])\n\u001B[32m      5\u001B[39m pri = cm.read_table(pop_metrics_dir / CFG.PATHS[\u001B[33m\"\u001B[39m\u001B[33mprior_commitments\u001B[39m\u001B[33m\"\u001B[39m])\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\PythonProject\\external_repos\\population_metrics\\compute_metrics.py:53\u001B[39m, in \u001B[36mread_table\u001B[39m\u001B[34m(path)\u001B[39m\n\u001B[32m     51\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Read CSV/XLSX from local path or HTTP(S).\"\"\"\u001B[39;00m\n\u001B[32m     52\u001B[39m path = _to_raw_github_url(path)\n\u001B[32m---> \u001B[39m\u001B[32m53\u001B[39m p = \u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlower\u001B[49m()\n\u001B[32m     54\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m p.startswith((\u001B[33m\"\u001B[39m\u001B[33mhttp://\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mhttps://\u001B[39m\u001B[33m\"\u001B[39m)):\n\u001B[32m     55\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m p.endswith((\u001B[33m\"\u001B[39m\u001B[33m.xlsx\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33m.xls\u001B[39m\u001B[33m\"\u001B[39m)):\n",
      "\u001B[31mAttributeError\u001B[39m: 'WindowsPath' object has no attribute 'lower'"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T16:58:24.139307Z",
     "start_time": "2025-11-11T16:58:24.118546Z"
    }
   },
   "source": [
    "# Get sample of IDs for demo\n",
    "all_ids = demo[CFG.COLS[\"id\"]].astype(str).unique().tolist()\n",
    "random.seed(42)\n",
    "sample_ids = random.sample(all_ids, min(1000, len(all_ids)))  # Small sample for notebook\n",
    "\n",
    "print(f\"Processing {len(sample_ids):,} individuals...\")"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'demo' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[17]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# Get sample of IDs for demo\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m all_ids = \u001B[43mdemo\u001B[49m[CFG.COLS[\u001B[33m\"\u001B[39m\u001B[33mid\u001B[39m\u001B[33m\"\u001B[39m]].astype(\u001B[38;5;28mstr\u001B[39m).unique().tolist()\n\u001B[32m      3\u001B[39m random.seed(\u001B[32m42\u001B[39m)\n\u001B[32m      4\u001B[39m sample_ids = random.sample(all_ids, \u001B[38;5;28mmin\u001B[39m(\u001B[32m1000\u001B[39m, \u001B[38;5;28mlen\u001B[39m(all_ids)))  \u001B[38;5;66;03m# Small sample for notebook\u001B[39;00m\n",
      "\u001B[31mNameError\u001B[39m: name 'demo' is not defined"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each person\n",
    "rows = []\n",
    "\n",
    "for uid in tqdm(sample_ids[:100]):  # Just first 100 for demo\n",
    "    try:\n",
    "        feats, aux = cm.compute_features(uid, demo, cur, pri, CFG.OFFENSE_LISTS)\n",
    "        present = feats.keys() & CFG.METRIC_WEIGHTS.keys()\n",
    "        score = sm.suitability_score_named(feats, CFG.METRIC_WEIGHTS)\n",
    "        score_out_of = sum(abs(CFG.METRIC_WEIGHTS[k]) for k in present)\n",
    "        \n",
    "        rows.append({\n",
    "            CFG.COLS[\"id\"]: uid,\n",
    "            **feats,\n",
    "            \"score\": score,\n",
    "            \"score_out_of\": score_out_of\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {uid}: {e}\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(rows)\n",
    "print(f\"\\nProcessed {len(results_df):,} individuals successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results (if you ran via script)\n",
    "# OR use results_df from above if you ran in notebook\n",
    "\n",
    "output_path = project_root / \"outputs\" / \"population_metrics_demo.csv\"\n",
    "\n",
    "if output_path.exists():\n",
    "    results_df = pd.read_csv(output_path)\n",
    "    print(f\"Loaded results from: {output_path}\")\n",
    "else:\n",
    "    print(\"Using results from notebook execution\")\n",
    "\n",
    "print(f\"\\nResults shape: {results_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "results_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of suitability scores\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(results_df['score'], bins=30, edgecolor='black')\n",
    "plt.xlabel('Suitability Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Suitability Scores')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for zeros (potential issue)\n",
    "zero_scores = (results_df['score'] == 0).sum()\n",
    "print(f\"Individuals with score = 0: {zero_scores} ({zero_scores/len(results_df)*100:.1f}%)\")\n",
    "\n",
    "if zero_scores > len(results_df) * 0.5:\n",
    "    print(\"\\n⚠️ Warning: More than 50% have score = 0\")\n",
    "    print(\"This suggests offense classification may not be working properly\")\n",
    "else:\n",
    "    print(\"\\n✓ Score distribution looks reasonable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Diagnostic - Look at Specific Person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a specific person to examine\n",
    "example_id = results_df.iloc[0]['cdcno']\n",
    "print(f\"Examining person: {example_id}\\n\")\n",
    "\n",
    "# Their metrics\n",
    "person_metrics = results_df[results_df['cdcno'] == example_id]\n",
    "print(\"Calculated metrics:\")\n",
    "print(person_metrics.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Their raw data\n",
    "print(\"\\nRaw offense data:\")\n",
    "person_offenses = cur[cur[CFG.COLS['id']] == example_id][['offense_clean', 'offense description']]\n",
    "print(person_offenses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if offenses matched classification\n",
    "person_codes = person_offenses['offense_clean'].dropna().tolist()\n",
    "violent_codes = set(CFG.OFFENSE_LISTS.get('violent', []))\n",
    "nonviolent_codes = set(CFG.OFFENSE_LISTS.get('nonviolent', []))\n",
    "\n",
    "print(\"\\nClassification check:\")\n",
    "for code in person_codes:\n",
    "    if str(code) in violent_codes:\n",
    "        print(f\"  {code}: VIOLENT\")\n",
    "    elif str(code) in nonviolent_codes:\n",
    "        print(f\"  {code}: NONVIOLENT\")\n",
    "    else:\n",
    "        print(f\"  {code}: UNCLASSIFIED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**What we did:**\n",
    "1. ✓ Validated configuration settings\n",
    "2. ✓ Checked data files exist\n",
    "3. ✓ Verified offense classification\n",
    "4. ✓ Ran population metrics\n",
    "5. ✓ Reviewed results\n",
    "\n",
    "**Next steps:**\n",
    "- If scores look good → Run full dataset\n",
    "- If many zeros → Debug offense classification\n",
    "- Proceed to regression analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
